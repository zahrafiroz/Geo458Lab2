{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMNyq+ZCH6u1e0u0Xue56xw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahrafiroz/Geo458Lab2/blob/main/youtubecrawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCZxJ0DjMIlG"
      },
      "outputs": [],
      "source": [
        "# created on January 2024\n",
        "# @author:          Zahra Firoz\n",
        "# @email:           zahraf@uw.edu\n",
        "# @website:         https://\n",
        "# @organization:    Geo 458a Lab number 2, University of Washington, Seattle\n",
        "# @description:     A demo of collecting data from YouTube.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "sudo apt -y update\n",
        "sudo apt install -y wget curl unzip\n",
        "wget http://archive.ubuntu.com/ubuntu/pool/main/libu/libu2f-host/libu2f-udev_1.1.4-1_all.deb\n",
        "dpkg -i libu2f-udev_1.1.4-1_all.deb\n",
        "wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
        "dpkg -i google-chrome-stable_current_amd64.deb\n",
        "\n",
        "pip install selenium chromedriver_autoinstaller"
      ],
      "metadata": {
        "id": "9c_XyntWMuN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup # Import BeautifulSoup to parse the HTML.\n",
        "import time, datetime # Import time and datetime to record the time.\n",
        "import pandas as pd # Import pandas to create a dataframe, and it can save the dataframe as a csv file."
      ],
      "metadata": {
        "id": "blw1hoCVMunr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "\n",
        "chrome_options = Options() # Create an instance of Options so you can add arguments to the driver.\n",
        "chrome_options.add_argument('--headless') # Add an argument 'headless' to run Chrome in headless mode.\n",
        "chrome_options.add_argument('--no-sandbox') # Add an argument 'no-sandbox' to run Chrome in no-sandbox mode.\n",
        "chrome_options.add_argument('--disable-dev-shm-usage') # Add an argument 'disable-dev-shm-usage' to run Chrome in disable-dev-shm-usage mode.\n",
        "\n",
        "bot = webdriver.Chrome(options=chrome_options) # Create an instance of Chrome. Pass the argument 'options' to the constructor of Chrome."
      ],
      "metadata": {
        "id": "tRQNFKOvMx6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The url where the data will be collected from.\n",
        "url = \"https://www.youtube.com/results?search_query=afghanistan\"\n",
        "# Input the targeting url to the bot, and the bot will load data from the url.\n",
        "bot.get(url)"
      ],
      "metadata": {
        "id": "DwcGNPHAMz5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# An array to store all the video urls. If a video has been crawled, it would not be stored to the data frame.\n",
        "video_urls = []\n",
        "# An array to store the retrieved video details.\n",
        "results = []"
      ],
      "metadata": {
        "id": "5MW9vnWZM2IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# variable i indicates the number of times that scrolls down a web page. In practice, you might want to develop different\n",
        "# interaction approach to load and view the web pages.\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "    # it is very important to enable the bot take some rest, and then resume to work.\n",
        "    time.sleep(5)\n",
        "    # Let the bot scrolls down to the bottom of the content element, most of the time the bot needs to scroll down to the bottom of the page.\n",
        "    # like this statement: bot.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    bot.execute_script('window.scrollTo(0,  document.getElementById(\"content\").scrollHeight);')"
      ],
      "metadata": {
        "id": "i43N2LnHM6Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a document object model (DOM) from the raw source of the crawled web page.\n",
        "# Since you are processing a html page, 'html.parser' is chosen.\n",
        "soup = BeautifulSoup(bot.page_source, 'html.parser')\n",
        "\n",
        "# Capture all the video items using find_all or findAll method.\n",
        "# To view the information of the html elements you want to collect, you need to inspect the raw source using Chrome Inspector.\n",
        "videos = soup.find_all('ytd-video-renderer', class_=\"style-scope ytd-item-section-renderer\")[-20:] # 20 indicates only process the newly-acquired 20 entries.\n"
      ],
      "metadata": {
        "id": "SJ5ofY7YM8Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for video in videos:\n",
        "\n",
        "    # I prefer use the \"try-except\" statement to enable the program run without pausing due to unexpected errors.\n",
        "    try:\n",
        "      ...\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "id": "5feL1SccM_fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_url = video.find(\"a\", class_=\"yt-simple-endpoint style-scope ytd-video-renderer\").attrs[\"href\"]\n",
        "user_url = video.find(\"a\", class_=\"yt-simple-endpoint style-scope yt-formatted-string\").attrs[\"href\"]\n",
        "username = video.find(\"a\", class_=\"yt-simple-endpoint style-scope yt-formatted-string\").text\n",
        "title = video.find(\"yt-formatted-string\", class_=\"style-scope ytd-video-renderer\").text\n",
        "metadata_items = video.find_all(\"span\", class_=\"inline-metadata-item style-scope ytd-video-meta-block\")\n",
        "view_num = metadata_items[0].text.replace(\" views\", \"\")\n",
        "created_at = metadata_items[1].text.replace(\" ago\", \"\")\n",
        "shortdesc = video.find(\"yt-formatted-string\", class_=\"metadata-snippet-text style-scope ytd-video-renderer\").text\n",
        "collected_at = datetime.datetime.now()"
      ],
      "metadata": {
        "id": "wSDjMGnpNC0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row = {'video_url': video_url,\n",
        "        'user_url': user_url,\n",
        "        'username': username,\n",
        "        'title': title,\n",
        "        'view_num': view_num,\n",
        "        'created_at': created_at,\n",
        "        'shortdesc': shortdesc,\n",
        "        'collected_at': collected_at}\n",
        "\n",
        "\n",
        "if video_url in video_urls:\n",
        "    print(\"this video has already been added.\")\n",
        "else:\n",
        "    print(row)\n",
        "    results.append(row)"
      ],
      "metadata": {
        "id": "l_n0s0UZNE87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# terminate the bot object.\n",
        "bot.close()"
      ],
      "metadata": {
        "id": "U5ALYMgDNGu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the results as a pandas dataframe\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# notify the completion of the crawling in the console.\n",
        "print(\"the crawling task is finished.\")"
      ],
      "metadata": {
        "id": "0adYljYMNIcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data on to Google Drive\n",
        "from google.colab import drive\n",
        "# Mount your Drive to the Colab VM.\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# the file path where to store the output csv on google drive\n",
        "output_file = '/gdrive/My Drive/videos.csv'\n",
        "\n",
        "# Save the dataframe as a csv file\n",
        "df.to_csv(output_file, index=False)"
      ],
      "metadata": {
        "id": "NPdQN8j7NKYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the csv to your local computer\n",
        "from google.colab import files\n",
        "files.download(output_file)\n",
        "print(\"the csv has been downloaded to your local computer. The program has been completed successfully.\")"
      ],
      "metadata": {
        "id": "F6M5wNSeNMQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The url where the data will be collected from.\n",
        "search_terms = [\"Geography\", \"GIS\", \"Maps\"]\n",
        "for search_term in search_terms:\n",
        "    url = \"https://www.youtube.com/results?search_query=\" + search_term.replace(\" \", \"+\")\n",
        "    # Input the targeting url to the bot, and the bot will load data from the url.\n",
        "    bot.get(url)"
      ],
      "metadata": {
        "id": "OdT12I0WNOwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The url where the data will be collected from.\n",
        "locations = [\n",
        "    {\"name\": \"Seattle\", \"lat\": 47.6062, \"lng\": -122.3321},\n",
        "    {\"name\": \"Tacoma\", \"lat\": 47.2529, \"lng\": -122.4443},\n",
        "    {\"name\": \"Olympia\", \"lat\": 47.0379, \"lng\": -122.9007}\n",
        "]\n",
        "for location in locations:\n",
        "    url = \"https://www.youtube.com/results?search_query=\" + location[\"name\"].replace(\" \", \"+\")\n",
        "    # Input the targeting url to the bot, and the bot will load data from the url.\n",
        "    bot.get(url)\n"
      ],
      "metadata": {
        "id": "J22ncewiOHkT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}