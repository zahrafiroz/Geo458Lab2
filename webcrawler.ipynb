{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONWc+wq8h+51H/FxSqUAKG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahrafiroz/Geo458Lab2/blob/main/webcrawler.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# created on January 2024\n",
        "# @author:          Zahra Firoz\n",
        "# @email:           zahraf@uw.edu\n",
        "# @website:         https://https://github.com/zahrafiroz/Geo458Lab2\n",
        "# @organization:    Geo 458a Lab number 2, University of Washington, Seattle\n",
        "# @description:     A demo of collecting data from Twitter regarding Afghanistan Geography.\n",
        "\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "# the file path where to store the output csv on google drive\n",
        "output_file = '/gdrive/My Drive/twsearch-result.csv'\n",
        "\n",
        "# Apply for your own Twitter API keys at https://developer.twitter.com/en/apply-for-access\n",
        "consumer_key = \"your_consumer_key\"\n",
        "consumer_secret = \"your_consumer_secret\"\n",
        "access_token = \"your_access_token\"\n",
        "access_token_secret = \"your_access_token_secret\"\n",
        "\n",
        "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "auth.set_access_token(access_token, access_token_secret)\n",
        "api = tweepy.API(auth, wait_on_rate_limit=True)\n",
        "# Define the search term and the date_since date as variables\n",
        "search_words = \"Afghanistan\" \"Geography\" \"GIS\"\n",
        "# make sure there is no space between lat, long and the radius.\n",
        "location = \"34.543896,69.160652,15.8mi\"\n",
        "#location set to Kabul Afghanistan\n",
        "date_since = \"2020-6-9\"\n",
        "# read the Twitter API document to look for other ways to customize your queries.\n",
        "# refer to https://developer.twitter.com/en/docs/twitter-api/v1/rules-and-filtering/search-operators\n",
        "# for example: you can ignore all the retweets by #wildfires -filter:retweets\n",
        "# Geolocalization: the search operator “near” isn’t available in the API, but there is a more precise way to restrict\n",
        "# your query by a given location using the geocode parameter specified with the template “latitude,longitude,radius”,\n",
        "# for example, “47.6138893,-122.3107869,10mi” (capitol hill at Seattle). When conducting geo searches, the search API will first attempt to find Tweets、\n",
        "# which have lat/long within the queried geocode, and in case of not having success, it will attempt to find Tweets created\n",
        "# by users whose profile location can be reverse geocoded into a lat/long within the queried geocode, meaning that is possible\n",
        "# to receive Tweets which do not include lat/long information.\n",
        "\n",
        "\n",
        "# Collect tweets\n",
        "# tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since).items(100)\n",
        "tweets = tweepy.Cursor(api.search, q=search_words, geocode=location, lang=\"en\", since=date_since).items(1000)\n",
        "\n",
        "# create an array to store the result\n",
        "result = []\n",
        "\n",
        "# Iterate and print tweets\n",
        "for tweet in tweets:\n",
        "    row = {\n",
        "        'username': tweet.author.name,\n",
        "        'userid': tweet.author.id,\n",
        "        'profile_location': tweet.author.location,\n",
        "        'created_at': str(tweet.created_at),\n",
        "        'text': tweet.text,\n",
        "        'retweet_count': tweet.retweet_count,\n",
        "        'source': tweet.source,\n",
        "        'coordinates': tweet.coordinates\n",
        "    }\n",
        "    result.append(row)\n",
        "    print(row)\n",
        "\n",
        "# Store the results as a pandas dataframe\n",
        "df = pd.DataFrame(result)\n",
        "\n",
        "# notify the completion of the crawling in the console.\n",
        "print(\"the crawling task is finished.\")\n",
        "# Create data on to Google Drive\n",
        "from google.colab import drive\n",
        "# Mount your Drive to the Colab VM.\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "df.to_csv(output_file, index=False)\n",
        "# download the csv to your local computer\n",
        "from google.colab import files\n",
        "files.download(output_file)\n",
        "print(\"the csv has been downloaded to your local computer. The program has been completed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "s_eFAQjklEi7",
        "outputId": "d46ffa3a-b356-4d19-a957-4bbfc107052e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'API' object has no attribute 'search'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-a6b687a7ba2e>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# Collect tweets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# tweets = tweepy.Cursor(api.search, q=search_words, lang=\"en\", since=date_since).items(100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mtweets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtweepy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearch_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeocode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msince\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdate_since\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# create an array to store the result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'API' object has no attribute 'search'"
          ]
        }
      ]
    }
  ]
}